{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc0e02-1ac3-4516-8e07-aaa5d2cfe9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# print(\"sys.executable:\", sys.executable)\n",
    "# print(\"sys.version:\", sys.version)\n",
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read();\n",
    "print(\"total number of characters:\",len((raw_text)))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec5ae22-fb4f-4193-98a4-c33821f4630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 1)------------>\n",
    "import re\n",
    "text=\"Hello, world. This, is a test.\"\n",
    "result=re.split(r'(\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e006a714-e732-4ec6-972a-23f4d7e6ecd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=re.split(r'([,.]|\\s)',text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2a876-da50-41bb-a49b-d546b3214f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result =[item for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1721b9f-9615-4b5c-967d-219a5cbe3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"hello, world. Is this-- a test?\"\n",
    "result= re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "result=[item for item in result if item.strip()]\n",
    "print(result)\n",
    "#example_end 1)------------------->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6f445-778c-4188-bb44-12b5be217ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed=re.split(r'([,.:;?_!\"()\\']|--|\\s)',raw_text)\n",
    "preprocessed=[item for item in preprocessed if item.strip()]\n",
    "print(preprocessed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a157e04a-2724-47c8-8e79-00df842016a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225493af-3a15-42bf-b5f4-a4bc0135db3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=sorted(set(preprocessed))\n",
    "vocab_size=len(all_words)\n",
    "\n",
    "print(vocab_size) #no duplicates so len will be less than preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd7c582-a105-4fe0-b30a-95ab3706e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab ={token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1ca9b5-e29d-4291-99fd-f8127531c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i>=50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe2805c-3faa-4d2e-839e-f5f312f12b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed=[item for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids;\n",
    "    def decode(self,ids):\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        #replacing spacs before the specified punctuations\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec639b71-1623-4bd6-be61-c2f76853aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example 2)-----------> start\n",
    "tokenizer=SimpleTokenizerV1(vocab);\n",
    "text=\"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids=tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525b1ffe-e5a9-4ff1-9345-3521b1f2b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(ids)\n",
    "#example 2)---------------------> end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51adf23f-9d7c-4166-ab46-80411cd44c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding special context tokens\n",
    "all_tokens=sorted(list(set(preprocessed)))\n",
    "all_tokens.extend([\"<|extendoftext|>\",\"<|unk|>\"])\n",
    "\n",
    "vocab={token:integer for integer,token in enumerate(all_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6208d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d13ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#version 2 of tokenizer where we are replaceing unknown words in text to <|unk|> so that a number can be assigned to it encoder\n",
    "class SimpleTokenizerV2:\n",
    "    def __init__(self,vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={i:s for s,i in vocab.items()}\n",
    "    def encode(self,text):\n",
    "        preprocessed=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
    "        preprocessed=[item for item in preprocessed if item.strip()]\n",
    "        preprocessed=[\n",
    "            item if item in self.str_to_int\n",
    "            else \"<|unk|>\" for item in preprocessed\n",
    "        ]\n",
    "        ids=[self.str_to_int[s] for s in preprocessed]\n",
    "        return ids;\n",
    "    def decode(self,ids):\n",
    "        text=\" \".join([self.int_to_str[i] for i in ids])\n",
    "        #replacing spacs before the specified punctuations\n",
    "        text=re.sub(r'\\s+([,.?!\"()\\'])',r'\\1',text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ca1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=SimpleTokenizerV2(vocab)\n",
    "text1=\"Hello, do you like tea?\"\n",
    "text2=\"In the sunlit terraces of the palace\"\n",
    "\n",
    "text=\" <|endoftext|>\".join((text1,text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63647a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ef2444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import importlib.metadata\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258bdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9306e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "    \"of someunknownPlace\"\n",
    ")\n",
    "integers = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7c47c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings =tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a2685",
   "metadata": {},
   "outputs": [],
   "source": [
    "integers=tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()\n",
    "enc_text=tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample=enc_text[50:]\n",
    "print(enc_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d527b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size=4 #length of the input \n",
    "#the model looks at 4 words and predicts next four words\n",
    "x=enc_sample[:context_size]\n",
    "y=enc_sample[1:context_size+1]\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:      {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ce63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context=enc_sample[:i]\n",
    "    desired=enc_sample[i]\n",
    "\n",
    "    print(context,\"----->\",desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c32065",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,context_size+1):\n",
    "    context=enc_sample[:i]\n",
    "    desired=enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context),\"----->\",tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a997b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "   def __init__(self,txt,tokenizer,max_length,stride):\n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[]\n",
    "        \n",
    "        token_ids=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        for i in range(0,len(token_ids)-max_length,stride):\n",
    "            input_chunk=token_ids[i:i+max_length]\n",
    "            target_chunk=token_ids[i+1:i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "   def __len__(self):\n",
    "       return len(self.input_ids)\n",
    "\n",
    "   def __getitem__(self,idx):\n",
    "       return self.input_ids[idx],self.target_ids[idx]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
    "\n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    dataset=GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
    "\n",
    "    dataloader=DataLoader(\n",
    "        dataset,batch_size=batch_size,shuffle=shuffle,drop_last=drop_last,num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899598c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Pytorch version:\",torch.__version__)\n",
    "dataloader=create_dataloader_v1(\n",
    "    raw_text,batch_size=1,max_length=4,stride=1,shuffle=False\n",
    ")\n",
    "\n",
    "data_iter=iter(dataloader)\n",
    "first_batch=next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2a115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_batch=next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca47045",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=create_dataloader_v1(raw_text,batch_size=8,max_length=4,stride=4,shuffle=False)\n",
    "\n",
    "data_iter=iter(dataloader)\n",
    "inputs,targets=next(data_iter)\n",
    "print(\"Inputs:\\n\",inputs)\n",
    "print(\"\\nTargets:\\n\",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5467417",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size=6\n",
    "output_dim=3\n",
    "torch.manual_seed(123)\n",
    "embedding_layer=torch.nn.Embedding(vocab_size,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca25559",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embedding_layer(torch.tensor([3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ba57dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--positional emberding\n",
    "vocab_size=50257\n",
    "output_dim=256\n",
    "\n",
    "token_embedding_layer=torch.nn.Embedding(vocab_size,output_dim)\n",
    "print(token_embedding_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a158d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length=4\n",
    "dataloader=create_dataloader_v1(\n",
    "    raw_text,batch_size=8,max_length=max_length,\n",
    "    stride=max_length,shuffle=False\n",
    ")\n",
    "data_iter=iter(dataloader)\n",
    "inputs,targets=next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2614fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token IDs:\\n\",inputs)\n",
    "print(\"\\nInputs shpae:\\n\",inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f453af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings=token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f49b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length=max_length\n",
    "pos_embedding_layer=torch.nn.Embedding(context_length,output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b55fff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embeddings=token_embeddings+pos_embeddings\n",
    "print(input_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simplified self attention mechanism\n",
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d049f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Corresponding words\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']\n",
    "\n",
    "# Extract x, y, z coordinates\n",
    "x_coords = inputs[:, 0].numpy()\n",
    "y_coords = inputs[:, 1].numpy()\n",
    "z_coords = inputs[:, 2].numpy()\n",
    "\n",
    "# Create 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot each point and annotate with corresponding word\n",
    "for x, y, z, word in zip(x_coords, y_coords, z_coords, words):\n",
    "    ax.scatter(x, y, z)\n",
    "    ax.text(x, y, z, word, fontsize=10)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b06191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D plot with vectors from origin to each point, using different colors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Define a list of colors for the vectors\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y']\n",
    "\n",
    "# Plot each vector with a different color and annotate with the corresponding word\n",
    "for (x, y, z, word, color) in zip(x_coords, y_coords, z_coords, words, colors):\n",
    "    # Draw vector from origin to the point (x, y, z) with specified color and smaller arrow length ratio\n",
    "    ax.quiver(0, 0, 0, x, y, z, color=color, arrow_length_ratio=0.05)\n",
    "    ax.text(x, y, z, word, fontsize=10, color=color)\n",
    "\n",
    "# Set labels for axes\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Set plot limits to keep arrows within the plot boundaries\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "ax.set_zlim([0, 1])\n",
    "\n",
    "plt.title('3D Plot of Word Embeddings with Colored Vectors')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=inputs[1]\n",
    "attn_scores_2=torch.empty(inputs.shape[0])\n",
    "for i,x_i in enumerate(inputs):\n",
    "    attn_scores_2[i]=torch.dot(x_i,query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_scpre_normal=attn_scores_2/attn_scores_2.sum()\n",
    "print(\"att weights:\",att_scpre_normal)\n",
    "print(\"sum:\",att_scpre_normal.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc4438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4db308",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba652fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a91bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0fb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceb9408",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ddff54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2e7f97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152600b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249569b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339b966",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bc2267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feab8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b47df89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b97826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff47de68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
