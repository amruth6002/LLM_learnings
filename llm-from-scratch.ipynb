{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff9f91df",
   "metadata": {
    "papermill": {
     "duration": 0.006879,
     "end_time": "2025-08-23T20:46:12.418151",
     "exception": false,
     "start_time": "2025-08-23T20:46:12.411272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reading the text verdict.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da77dbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:12.430398Z",
     "iopub.status.busy": "2025-08-23T20:46:12.430148Z",
     "iopub.status.idle": "2025-08-23T20:46:18.394717Z",
     "shell.execute_reply": "2025-08-23T20:46:18.394071Z"
    },
    "papermill": {
     "duration": 5.97203,
     "end_time": "2025-08-23T20:46:18.396041",
     "exception": false,
     "start_time": "2025-08-23T20:46:12.424011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "# url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "# file_path = \"the-verdict.txt\"\n",
    "\n",
    "# urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "# with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     raw_text = f.read()\n",
    "\n",
    "# print(\"Total number of characters:\", len(raw_text))\n",
    "# print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8a9c7a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:18.409674Z",
     "iopub.status.busy": "2025-08-23T20:46:18.409367Z",
     "iopub.status.idle": "2025-08-23T20:46:18.412624Z",
     "shell.execute_reply": "2025-08-23T20:46:18.412070Z"
    },
    "papermill": {
     "duration": 0.011601,
     "end_time": "2025-08-23T20:46:18.413638",
     "exception": false,
     "start_time": "2025-08-23T20:46:18.402037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! pip3 install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cc4427",
   "metadata": {
    "papermill": {
     "duration": 0.00535,
     "end_time": "2025-08-23T20:46:18.425152",
     "exception": false,
     "start_time": "2025-08-23T20:46:18.419802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Using byte-pair encoding for encoding text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e8085e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:18.436810Z",
     "iopub.status.busy": "2025-08-23T20:46:18.436558Z",
     "iopub.status.idle": "2025-08-23T20:46:18.513702Z",
     "shell.execute_reply": "2025-08-23T20:46:18.512954Z"
    },
    "papermill": {
     "duration": 0.084184,
     "end_time": "2025-08-23T20:46:18.514726",
     "exception": false,
     "start_time": "2025-08-23T20:46:18.430542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a20fa537",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:18.526862Z",
     "iopub.status.busy": "2025-08-23T20:46:18.526399Z",
     "iopub.status.idle": "2025-08-23T20:46:19.455123Z",
     "shell.execute_reply": "2025-08-23T20:46:19.454501Z"
    },
    "papermill": {
     "duration": 0.936171,
     "end_time": "2025-08-23T20:46:19.456501",
     "exception": false,
     "start_time": "2025-08-23T20:46:18.520330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ad314b",
   "metadata": {
    "papermill": {
     "duration": 0.00528,
     "end_time": "2025-08-23T20:46:19.467518",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.462238",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data-Loader (loads the data given number of batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0641efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:19.479274Z",
     "iopub.status.busy": "2025-08-23T20:46:19.479040Z",
     "iopub.status.idle": "2025-08-23T20:46:19.482463Z",
     "shell.execute_reply": "2025-08-23T20:46:19.481946Z"
    },
    "papermill": {
     "duration": 0.010662,
     "end_time": "2025-08-23T20:46:19.483478",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.472816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# class GPTDatasetV1(Dataset):\n",
    "#     def __init__(self, txt, tokenizer, max_length, stride):\n",
    "#         self.input_ids = []\n",
    "#         self.target_ids = []\n",
    "\n",
    "#         # Tokenize the entire text\n",
    "#         token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "#         # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "#         for i in range(0, len(token_ids) - max_length, stride):\n",
    "#             input_chunk = token_ids[i:i + max_length]\n",
    "#             target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "#             self.input_ids.append(torch.tensor(input_chunk))\n",
    "#             self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.input_ids)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8acfab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:19.495388Z",
     "iopub.status.busy": "2025-08-23T20:46:19.495211Z",
     "iopub.status.idle": "2025-08-23T20:46:19.498281Z",
     "shell.execute_reply": "2025-08-23T20:46:19.497787Z"
    },
    "papermill": {
     "duration": 0.010025,
     "end_time": "2025-08-23T20:46:19.499187",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.489162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "#                          stride=128, shuffle=True, drop_last=True,\n",
    "#                          num_workers=0):\n",
    "\n",
    "#     # Initialize the tokenizer\n",
    "#     tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "#     # Create dataset\n",
    "#     dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "#     # Create dataloader\n",
    "#     dataloader = DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=shuffle,\n",
    "#         drop_last=drop_last,\n",
    "#         num_workers=num_workers\n",
    "#     )\n",
    "\n",
    "#     return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14c86f",
   "metadata": {
    "papermill": {
     "duration": 0.005215,
     "end_time": "2025-08-23T20:46:19.510071",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.504856",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Multi-head attention (version 2 - improved) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfa984b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:19.521496Z",
     "iopub.status.busy": "2025-08-23T20:46:19.521323Z",
     "iopub.status.idle": "2025-08-23T20:46:19.529069Z",
     "shell.execute_reply": "2025-08-23T20:46:19.528572Z"
    },
    "papermill": {
     "duration": 0.014717,
     "end_time": "2025-08-23T20:46:19.530060",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.515343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "        \n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        \n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "         \n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "        \n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75b0b85",
   "metadata": {
    "papermill": {
     "duration": 0.005337,
     "end_time": "2025-08-23T20:46:19.540784",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.535447",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Entire GPT model architecture implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d32ff0db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:19.552445Z",
     "iopub.status.busy": "2025-08-23T20:46:19.551949Z",
     "iopub.status.idle": "2025-08-23T20:46:19.555594Z",
     "shell.execute_reply": "2025-08-23T20:46:19.554946Z"
    },
    "papermill": {
     "duration": 0.010585,
     "end_time": "2025-08-23T20:46:19.556703",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.546118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30f3e00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:19.568367Z",
     "iopub.status.busy": "2025-08-23T20:46:19.567827Z",
     "iopub.status.idle": "2025-08-23T20:46:19.574139Z",
     "shell.execute_reply": "2025-08-23T20:46:19.573465Z"
    },
    "papermill": {
     "duration": 0.01312,
     "end_time": "2025-08-23T20:46:19.575194",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.562074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]), ## Expansion\n",
    "            GELU(), ## Activation\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]), ## Contraction\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a8147f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:19.586598Z",
     "iopub.status.busy": "2025-08-23T20:46:19.586387Z",
     "iopub.status.idle": "2025-08-23T20:46:19.591143Z",
     "shell.execute_reply": "2025-08-23T20:46:19.590665Z"
    },
    "papermill": {
     "duration": 0.011652,
     "end_time": "2025-08-23T20:46:19.592104",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.580452",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        # 2*4*768\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x\n",
    "        # 2*4*768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bac061c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:19.604051Z",
     "iopub.status.busy": "2025-08-23T20:46:19.603364Z",
     "iopub.status.idle": "2025-08-23T20:46:19.608696Z",
     "shell.execute_reply": "2025-08-23T20:46:19.608188Z"
    },
    "papermill": {
     "duration": 0.012259,
     "end_time": "2025-08-23T20:46:19.609771",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.597512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "075e2955",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:19.621503Z",
     "iopub.status.busy": "2025-08-23T20:46:19.621068Z",
     "iopub.status.idle": "2025-08-23T20:46:21.067314Z",
     "shell.execute_reply": "2025-08-23T20:46:21.066650Z"
    },
    "papermill": {
     "duration": 1.453334,
     "end_time": "2025-08-23T20:46:21.068545",
     "exception": false,
     "start_time": "2025-08-23T20:46:19.615211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GPTModel(GPT_CONFIG_124M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7282603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.080797Z",
     "iopub.status.busy": "2025-08-23T20:46:21.080301Z",
     "iopub.status.idle": "2025-08-23T20:46:21.084892Z",
     "shell.execute_reply": "2025-08-23T20:46:21.084118Z"
    },
    "papermill": {
     "duration": 0.011704,
     "end_time": "2025-08-23T20:46:21.085973",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.074269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "317252a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.098442Z",
     "iopub.status.busy": "2025-08-23T20:46:21.098010Z",
     "iopub.status.idle": "2025-08-23T20:46:21.102176Z",
     "shell.execute_reply": "2025-08-23T20:46:21.101349Z"
    },
    "papermill": {
     "duration": 0.011443,
     "end_time": "2025-08-23T20:46:21.103359",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.091916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89021f76",
   "metadata": {
    "papermill": {
     "duration": 0.005372,
     "end_time": "2025-08-23T20:46:21.114860",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.109488",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76fb4eb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.126582Z",
     "iopub.status.busy": "2025-08-23T20:46:21.126380Z",
     "iopub.status.idle": "2025-08-23T20:46:21.129582Z",
     "shell.execute_reply": "2025-08-23T20:46:21.129070Z"
    },
    "papermill": {
     "duration": 0.010214,
     "end_time": "2025-08-23T20:46:21.130530",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.120316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Train/validation ratio\n",
    "# train_ratio = 0.90\n",
    "# split_idx = int(train_ratio * len(raw_text))\n",
    "# train_data = raw_text[:split_idx]\n",
    "# val_data = raw_text[split_idx:]\n",
    "\n",
    "\n",
    "# torch.manual_seed(123)\n",
    "\n",
    "# train_loader = create_dataloader_v1(\n",
    "#     train_data,\n",
    "#     batch_size=2,\n",
    "#     max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "#     stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "#     drop_last=True,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0\n",
    "# )\n",
    "\n",
    "# val_loader = create_dataloader_v1(\n",
    "#     val_data,\n",
    "#     batch_size=2,\n",
    "#     max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "#     stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "#     drop_last=False,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "266c1f2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.142353Z",
     "iopub.status.busy": "2025-08-23T20:46:21.142149Z",
     "iopub.status.idle": "2025-08-23T20:46:21.145422Z",
     "shell.execute_reply": "2025-08-23T20:46:21.144899Z"
    },
    "papermill": {
     "duration": 0.010317,
     "end_time": "2025-08-23T20:46:21.146339",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.136022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "#     input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "#     logits = model(input_batch)\n",
    "#     loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "#     total_loss = 0.\n",
    "#     if len(data_loader) == 0:\n",
    "#         return float(\"nan\")\n",
    "#     elif num_batches is None:\n",
    "#         num_batches = len(data_loader)\n",
    "#     else:\n",
    "#         # Reduce the number of batches to match the total number of batches in the data loader\n",
    "#         # if num_batches exceeds the number of batches in the data loader\n",
    "#         num_batches = min(num_batches, len(data_loader))\n",
    "#     for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "#         if i < num_batches:\n",
    "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "#             total_loss += loss.item()\n",
    "#         else:\n",
    "#             break\n",
    "#     return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fea4949",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.159277Z",
     "iopub.status.busy": "2025-08-23T20:46:21.159100Z",
     "iopub.status.idle": "2025-08-23T20:46:21.162101Z",
     "shell.execute_reply": "2025-08-23T20:46:21.161584Z"
    },
    "papermill": {
     "duration": 0.010175,
     "end_time": "2025-08-23T20:46:21.163090",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.152915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "#         val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "#     model.train()\n",
    "#     return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b31c3f",
   "metadata": {
    "papermill": {
     "duration": 0.005502,
     "end_time": "2025-08-23T20:46:21.174048",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.168546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47c9cc6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.185742Z",
     "iopub.status.busy": "2025-08-23T20:46:21.185536Z",
     "iopub.status.idle": "2025-08-23T20:46:21.188533Z",
     "shell.execute_reply": "2025-08-23T20:46:21.188052Z"
    },
    "papermill": {
     "duration": 0.009986,
     "end_time": "2025-08-23T20:46:21.189475",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.179489",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    \n",
    "#     for _ in range(max_new_tokens):\n",
    "        \n",
    "#         # Crop current context if it exceeds the supported context size\n",
    "#         # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
    "#         # then only the last 5 tokens are used as context\n",
    "#         idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "#         # Get the predictions\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(idx_cond) ### batch, n_tokens, vocab_size\n",
    "        \n",
    "#         # Focus only on the last time step\n",
    "#         # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
    "#         logits = logits[:, -1, :]  \n",
    "\n",
    "#         # Apply softmax to get probabilities\n",
    "#         probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "#         # Get the idx of the vocab entry with the highest probability value\n",
    "#         idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "#         # Append sampled index to the running sequence\n",
    "#         idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "#     return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bd1f556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.250057Z",
     "iopub.status.busy": "2025-08-23T20:46:21.249783Z",
     "iopub.status.idle": "2025-08-23T20:46:21.254075Z",
     "shell.execute_reply": "2025-08-23T20:46:21.253451Z"
    },
    "papermill": {
     "duration": 0.060081,
     "end_time": "2025-08-23T20:46:21.255191",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.195110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "# start_context = \"Every effort moves you\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf668b95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.267818Z",
     "iopub.status.busy": "2025-08-23T20:46:21.267591Z",
     "iopub.status.idle": "2025-08-23T20:46:21.270748Z",
     "shell.execute_reply": "2025-08-23T20:46:21.270211Z"
    },
    "papermill": {
     "duration": 0.010481,
     "end_time": "2025-08-23T20:46:21.271707",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.261226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "#     model.eval()\n",
    "#     context_size = model.pos_emb.weight.shape[0]\n",
    "#     encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "#     with torch.no_grad():\n",
    "#         token_ids = generate_text_simple(\n",
    "#             model=model, idx=encoded,\n",
    "#             max_new_tokens=50, context_size=context_size\n",
    "#         )\n",
    "#     decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "#     print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
    "#     model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bcf3df5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.283886Z",
     "iopub.status.busy": "2025-08-23T20:46:21.283666Z",
     "iopub.status.idle": "2025-08-23T20:46:21.287079Z",
     "shell.execute_reply": "2025-08-23T20:46:21.286545Z"
    },
    "papermill": {
     "duration": 0.010677,
     "end_time": "2025-08-23T20:46:21.288106",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.277429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "#                        eval_freq, eval_iter, start_context, tokenizer):\n",
    "#     # Initialize lists to track losses and tokens seen\n",
    "#     train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "#     tokens_seen, global_step = 0, -1\n",
    "\n",
    "#     # Main training loop\n",
    "#     for epoch in range(num_epochs):\n",
    "#         model.train()  # Set model to training mode\n",
    "        \n",
    "#         for input_batch, target_batch in train_loader:\n",
    "#             optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
    "#             loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "#             loss.backward() # Calculate loss gradients\n",
    "#             optimizer.step() # Update model weights using loss gradients\n",
    "#             tokens_seen += input_batch.numel() # Returns the total number of elements (or tokens) in the input_batch.\n",
    "#             global_step += 1\n",
    "\n",
    "#             # Optional evaluation step\n",
    "#             if global_step % eval_freq == 0: \n",
    "#                 train_loss, val_loss = evaluate_model(\n",
    "#                     model, train_loader, val_loader, device, eval_iter)\n",
    "#                 train_losses.append(train_loss)\n",
    "#                 val_losses.append(val_loss)\n",
    "#                 track_tokens_seen.append(tokens_seen)\n",
    "#                 print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "#                       f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "#         # Print a sample text after each epoch\n",
    "#         generate_and_print_sample(\n",
    "#             model, tokenizer, device, start_context\n",
    "#         )\n",
    "\n",
    "#     return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff33e6a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.300984Z",
     "iopub.status.busy": "2025-08-23T20:46:21.300467Z",
     "iopub.status.idle": "2025-08-23T20:46:21.303708Z",
     "shell.execute_reply": "2025-08-23T20:46:21.303107Z"
    },
    "papermill": {
     "duration": 0.010482,
     "end_time": "2025-08-23T20:46:21.304799",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.294317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Note:\n",
    "# # Uncomment the following code to calculate the execution time\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.manual_seed(123)\n",
    "# model = GPTModel(GPT_CONFIG_124M)\n",
    "# model.to(device)\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "# num_epochs = 10\n",
    "# train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "#     model, train_loader, val_loader, optimizer, device,\n",
    "#     num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
    "#     start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    "# )\n",
    "\n",
    "# # Note:\n",
    "# # Uncomment the following code to show the execution time\n",
    "# end_time = time.time()\n",
    "# execution_time_minutes = (end_time - start_time) / 60\n",
    "# print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95fc120f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.316547Z",
     "iopub.status.busy": "2025-08-23T20:46:21.316369Z",
     "iopub.status.idle": "2025-08-23T20:46:21.319359Z",
     "shell.execute_reply": "2025-08-23T20:46:21.318836Z"
    },
    "papermill": {
     "duration": 0.010076,
     "end_time": "2025-08-23T20:46:21.320392",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.310316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "# def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "#     fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "\n",
    "#     # Plot training and validation loss against epochs\n",
    "#     ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "#     ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "#     ax1.set_xlabel(\"Epochs\")\n",
    "#     ax1.set_ylabel(\"Loss\")\n",
    "#     ax1.legend(loc=\"upper right\")\n",
    "#     ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
    "\n",
    "#     # Create a second x-axis for tokens seen\n",
    "#     ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
    "#     ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
    "#     ax2.set_xlabel(\"Tokens seen\")\n",
    "\n",
    "#     fig.tight_layout()  # Adjust layout to make room\n",
    "#     plt.savefig(\"loss-plot.pdf\")\n",
    "#     plt.show()\n",
    "\n",
    "# epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "# plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b166939",
   "metadata": {
    "papermill": {
     "duration": 0.005235,
     "end_time": "2025-08-23T20:46:21.331101",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.325866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Loading pre-trained open-ai weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8bdb08d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:21.342483Z",
     "iopub.status.busy": "2025-08-23T20:46:21.342316Z",
     "iopub.status.idle": "2025-08-23T20:46:26.349009Z",
     "shell.execute_reply": "2025-08-23T20:46:26.347992Z"
    },
    "papermill": {
     "duration": 5.013848,
     "end_time": "2025-08-23T20:46:26.350341",
     "exception": false,
     "start_time": "2025-08-23T20:46:21.336493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow>=2.15.0 tqdm>=4.66"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce95578c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:26.362948Z",
     "iopub.status.busy": "2025-08-23T20:46:26.362729Z",
     "iopub.status.idle": "2025-08-23T20:46:47.435341Z",
     "shell.execute_reply": "2025-08-23T20:46:47.434429Z"
    },
    "papermill": {
     "duration": 21.080327,
     "end_time": "2025-08-23T20:46:47.436595",
     "exception": false,
     "start_time": "2025-08-23T20:46:26.356268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-23 20:46:29.063239: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755981989.403628      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755981989.513854      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "tqdm version: 4.67.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"tqdm version:\", tqdm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3232aa69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:47.449731Z",
     "iopub.status.busy": "2025-08-23T20:46:47.449093Z",
     "iopub.status.idle": "2025-08-23T20:46:47.459863Z",
     "shell.execute_reply": "2025-08-23T20:46:47.459123Z"
    },
    "papermill": {
     "duration": 0.018173,
     "end_time": "2025-08-23T20:46:47.460987",
     "exception": false,
     "start_time": "2025-08-23T20:46:47.442814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests  # Make sure requests is installed\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path)\n",
    "\n",
    "    ## We have reached here until now ---> we have downloaded the files on our local machine.\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params\n",
    "\n",
    "def download_file(url, destination):\n",
    "    try:\n",
    "        # Send a GET request to download the file, disabling SSL verification\n",
    "        response = requests.get(url, stream=True, verify=False)\n",
    "\n",
    "        # Get the total file size from headers, defaulting to 0 if not present\n",
    "        file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "        # Check if file exists and has the same size\n",
    "        if os.path.exists(destination):\n",
    "            file_size_local = os.path.getsize(destination)\n",
    "            if file_size == file_size_local:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return\n",
    "\n",
    "        # Define the block size for reading the file\n",
    "        block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "        # Initialize the progress bar with total file size\n",
    "        progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "            # Open the destination file in binary write mode\n",
    "            with open(destination, \"wb\") as file:\n",
    "                # Iterate over the file data in chunks\n",
    "                for chunk in response.iter_content(block_size):\n",
    "                    progress_bar.update(len(chunk))  # Update progress bar\n",
    "                    file.write(chunk)  # Write the chunk to the file\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "        print(f\"Please check the URL: {url}\")\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "090ba49c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:46:47.473377Z",
     "iopub.status.busy": "2025-08-23T20:46:47.472751Z",
     "iopub.status.idle": "2025-08-23T20:47:09.594151Z",
     "shell.execute_reply": "2025-08-23T20:47:09.593481Z"
    },
    "papermill": {
     "duration": 22.128716,
     "end_time": "2025-08-23T20:47:09.595483",
     "exception": false,
     "start_time": "2025-08-23T20:46:47.466767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 127kiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 5.44MiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 166kiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [00:20<00:00, 24.6MiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 8.69MiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 3.33MiB/s]\n",
      "/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 3.20MiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bee653f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:09.625537Z",
     "iopub.status.busy": "2025-08-23T20:47:09.625284Z",
     "iopub.status.idle": "2025-08-23T20:47:09.629514Z",
     "shell.execute_reply": "2025-08-23T20:47:09.628890Z"
    },
    "papermill": {
     "duration": 0.02031,
     "end_time": "2025-08-23T20:47:09.630535",
     "exception": false,
     "start_time": "2025-08-23T20:47:09.610225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Parameter dictionary keys: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Settings:\", settings)\n",
    "print(\"Parameter dictionary keys:\", params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba50909c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:09.659457Z",
     "iopub.status.busy": "2025-08-23T20:47:09.658996Z",
     "iopub.status.idle": "2025-08-23T20:47:09.663645Z",
     "shell.execute_reply": "2025-08-23T20:47:09.662834Z"
    },
    "papermill": {
     "duration": 0.020057,
     "end_time": "2025-08-23T20:47:09.664742",
     "exception": false,
     "start_time": "2025-08-23T20:47:09.644685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.11010301 -0.03926672  0.03310751 ... -0.1363697   0.01506208\n",
      "   0.04531523]\n",
      " [ 0.04034033 -0.04861503  0.04624869 ...  0.08605453  0.00253983\n",
      "   0.04318958]\n",
      " [-0.12746179  0.04793796  0.18410145 ...  0.08991534 -0.12972379\n",
      "  -0.08785918]\n",
      " ...\n",
      " [-0.04453601 -0.05483596  0.01225674 ...  0.10435229  0.09783269\n",
      "  -0.06952604]\n",
      " [ 0.1860082   0.01665728  0.04611587 ... -0.09625227  0.07847701\n",
      "  -0.02245961]\n",
      " [ 0.05135201 -0.02768905  0.0499369  ...  0.00704835  0.15519823\n",
      "   0.12067825]]\n",
      "Token embedding weight tensor dimensions: (50257, 768)\n"
     ]
    }
   ],
   "source": [
    "print(params[\"wte\"])\n",
    "print(\"Token embedding weight tensor dimensions:\", params[\"wte\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db2e14d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:09.693036Z",
     "iopub.status.busy": "2025-08-23T20:47:09.692484Z",
     "iopub.status.idle": "2025-08-23T20:47:09.696529Z",
     "shell.execute_reply": "2025-08-23T20:47:09.696006Z"
    },
    "papermill": {
     "duration": 0.019178,
     "end_time": "2025-08-23T20:47:09.697544",
     "exception": false,
     "start_time": "2025-08-23T20:47:09.678366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define model configurations in a dictionary for compactness\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "# Copy the base configuration and update with specific model settings\n",
    "model_name = \"gpt2-small (124M)\"  # Example model name\n",
    "NEW_CONFIG = GPT_CONFIG_124M.copy()\n",
    "NEW_CONFIG.update(model_configs[model_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96988a48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:09.726256Z",
     "iopub.status.busy": "2025-08-23T20:47:09.726034Z",
     "iopub.status.idle": "2025-08-23T20:47:11.122131Z",
     "shell.execute_reply": "2025-08-23T20:47:11.121352Z"
    },
    "papermill": {
     "duration": 1.411778,
     "end_time": "2025-08-23T20:47:11.123248",
     "exception": false,
     "start_time": "2025-08-23T20:47:09.711470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NEW_CONFIG.update({\"context_length\": 1024, \"qkv_bias\": True})\n",
    "gpt = GPTModel(NEW_CONFIG)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cababdbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:11.152528Z",
     "iopub.status.busy": "2025-08-23T20:47:11.152296Z",
     "iopub.status.idle": "2025-08-23T20:47:11.155928Z",
     "shell.execute_reply": "2025-08-23T20:47:11.155355Z"
    },
    "papermill": {
     "duration": 0.019304,
     "end_time": "2025-08-23T20:47:11.156937",
     "exception": false,
     "start_time": "2025-08-23T20:47:11.137633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa640c56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:11.185500Z",
     "iopub.status.busy": "2025-08-23T20:47:11.185281Z",
     "iopub.status.idle": "2025-08-23T20:47:11.195650Z",
     "shell.execute_reply": "2025-08-23T20:47:11.194933Z"
    },
    "papermill": {
     "duration": 0.026078,
     "end_time": "2025-08-23T20:47:11.196785",
     "exception": false,
     "start_time": "2025-08-23T20:47:11.170707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    \n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        q_w, k_w, v_w = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        q_b, k_b, v_b = np.split(\n",
    "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
    "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.weight, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
    "            gpt.trf_blocks[b].att.out_proj.bias, \n",
    "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[0].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].weight, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
    "            gpt.trf_blocks[b].ff.layers[2].bias, \n",
    "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(\n",
    "            gpt.trf_blocks[b].norm1.scale, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(\n",
    "            gpt.trf_blocks[b].norm1.shift, \n",
    "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(\n",
    "            gpt.trf_blocks[b].norm2.scale, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(\n",
    "            gpt.trf_blocks[b].norm2.shift, \n",
    "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1b1b8ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:11.225073Z",
     "iopub.status.busy": "2025-08-23T20:47:11.224895Z",
     "iopub.status.idle": "2025-08-23T20:47:12.031751Z",
     "shell.execute_reply": "2025-08-23T20:47:12.030991Z"
    },
    "papermill": {
     "duration": 0.822413,
     "end_time": "2025-08-23T20:47:12.032983",
     "exception": false,
     "start_time": "2025-08-23T20:47:11.210570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa2fe5a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:12.063079Z",
     "iopub.status.busy": "2025-08-23T20:47:12.062847Z",
     "iopub.status.idle": "2025-08-23T20:47:12.068550Z",
     "shell.execute_reply": "2025-08-23T20:47:12.068014Z"
    },
    "papermill": {
     "duration": 0.021517,
     "end_time": "2025-08-23T20:47:12.069534",
     "exception": false,
     "start_time": "2025-08-23T20:47:12.048017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f84665a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-23T20:47:12.098837Z",
     "iopub.status.busy": "2025-08-23T20:47:12.098605Z",
     "iopub.status.idle": "2025-08-23T20:47:13.707888Z",
     "shell.execute_reply": "2025-08-23T20:47:13.706863Z"
    },
    "papermill": {
     "duration": 1.62557,
     "end_time": "2025-08-23T20:47:13.709281",
     "exception": false,
     "start_time": "2025-08-23T20:47:12.083711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " The secret of life is as elusive as the answer. What does the word \"life' want you to know—so your own life?\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "token_ids = generate(\n",
    "    model=gpt,\n",
    "    idx=text_to_token_ids(\"The secret of life is\", tokenizer).to(device),\n",
    "    max_new_tokens=25,\n",
    "    context_size=NEW_CONFIG[\"context_length\"],\n",
    "    top_k=50,\n",
    "    temperature=1.4\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 69.918243,
   "end_time": "2025-08-23T20:47:16.876589",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-23T20:46:06.958346",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
